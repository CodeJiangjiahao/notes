学习基础
	机器学习基础
		机器学习的分类与一般思路
	微积分基础
		泰勒公式, 导数, 梯度
	概率与统计基础
		概率公式, 常见分布, 常见统计量
	线性代数基础
		矩阵乘法的集合意义

机器学习分类			
	监督学习: 有目标的训练/学习	(机器学习算法: 分类预测, 回归分析)
		
			eg:用户点击/购买预测,房价预测	
	无监督学习:					(机器学习算法: 聚类, 关联规则)
			eg:邮件/新闻聚类
	强化学习: 					(机器学习算法: Q-learning, 时间差学习)
			eg:动态系统以及机器人控制
		
机器学习的一般思路
	得分函数
		z=w1*高+w2*富+w3*帅+w4*潜+w5*德 -----> sigmoid 函数 -----> P(z)=是女婿的概率
		
微积分: 
	夹逼定理: 当 x∈U(x0,r) 时, 有 g(x)<=f(x)<=h(x) 成立, 并且有 lim(x->x0)g(x) = A,lim(x->x0) h(x) = A, 那么 lim(x->x0) f(x) = A
	导数: 就是曲线的斜率,是曲线变化快慢的反应. 二阶导数是斜率变化快慢的反应, 表示曲线的凹凸性
		(u + v)′ = u′ + v′		(uv)′ = u′v + uv′
	泰勒公式 Taylor 公式 - Maclaurin 公式:
		f(x) = f(x0) + f′(x0) *(x-x0) + f′′(x0) * (x-x0)²/2! + ... + fⁿ(x0) * (x-x0)ⁿ/n! + Re(x)
		f(x) = f(0) + f′(0) * x + f′′(0) * x²/2! + ... + fⁿ(0) * xⁿ/n! + ο(xⁿ)
	方向导数: 如果函数 z=f(x,y) 在点 P(x,y) 是可微分的, 那么函数在该点延任一方向 L 的方向导数都存在, 且有 
				δf/δl=(δf/δx)*cos(φ) + (δf/δy)*cos(φ)	(φ 为 x 轴到方向 L 的转角)
	梯度: 梯度下降法
	凸函数: f(x) 在区间 [a,b] 上连续, 在 (a,b) 内二阶可导, 那么: 若 f′(x)>0, 则 f(x) 是凸的; 若 f′(x)<0, 则 f(x) 是凹的
	